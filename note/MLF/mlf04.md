# Feasibility of Learning 机器学习可行性
## 上节回顾

重点关注：具有`具体特征`的`填鸭式`(batch)的`监督式`数据的`二元分类或回归`

## Learning is Impossible? 学习似乎不可能 - 

- 第一种情景描述，根据不同特征进行分类，可以得到不同的分类结果

![分不准](/images/kx101_.png)

- 第二种情景描述，在已知数据集 D 上，g≈f 成立；在 D 以外的未知数据集上，g≈f 不一定成立

![局限](/images/kx102_.png)

![局限解答](/images/kx103_.png)

解释：

在 D 以外的数据中更接近目标函数似乎是做不到的，只能保证对 D 有很好的分类结果 - No Free Lunch 定理

> 通常一个学习算法比另一个更“优越”，只是针对特定的问题，先验信息，数据的分布，训练样本的数目，代价或奖励函数等

结论：**在训练集 D 以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的，除非加上一些假设条件**

## Probability to the Rescue 可能性推断

既然学习对样本以外未知的目标函数 f 很难推断，那么是否在其他场景可以推断一些未知呢？

![bin题设](/images/kx201_.png)

如何推断橙色珠子的概率？

![问题描述](/images/kx202_.png)

那么样本内的概率`ν`与样本外的概率`μ`有什么关系呢？
- 主观上，可能没关系(即使罐子橙珠非常多，拿出的样本中橙珠极其少)
- 概率上，很可能有关系(两者很可能相近)

![可能性与概率](/images/kx203_.png)

那么用公式([霍夫丁不等式](/note/SC/霍夫丁不等式.md))如何描述呢？

![霍夫丁](/images/kx204_.png)

显然，`ν ≈ μ`极有可能是正确的
- N 足够大，ϵ 足够小，从而上述概率(表示两者相近的可能性)足够大
- 不依赖未知的`μ`

![推断](/images/kx205_.png)

那么 N 足够大时，就极有可能根据已知的`ν`推断未知的`μ`

计算一下，假设 `μ=0.4`，那么从罐子中取一个大小为 10 的样本，其橙珠的概率`ν≤0.1`的概率最大是多少？

**0.33** ， 也就是说两者相差很大的可能性的上界也很小，反过来说，两者相差很小的可能性非常大

## Connection to Learning 转化到机器学习
如果样本 N 够大，且是独立同分布的，那么就可以根据**样本中 h(x)≠y_n 的概率**来推断**样本外 h(x)≠f(x) 的概率**

![推断](/images/kx300_.png)

也就是说可以根据**样本中 h(x)=y_n 的概率**来推断**样本外 h(x)=f(x) 的概率**

用错误率表示的话，就是很可能用样本中分类错误概率推算出样本外分类错误的概率

![错误率](/images/kx301_.png)

用公式表示就是

![公式](/images/kx302_.png)

**因此，当样本中分类出错率很小时，样本外分类出错率也会很小，从而很可能 h≈f**

验证一个 h
![验证h](/images/kx303_.png)

验证一个 h 流程

![验证流程](/images/kx304_.png)

## Connection to Real Learning 转化到实际学习

出现多个 h 

![多个h](/images/kx401_.png)

抛硬币游戏

![抛硬币游戏](/images/kx402_.png)

不好的数据和标签

![不好的数据和标签](/images/kx403_.png)

对多个 h 来说不好的数据 

![对多个h来说不好的数据](/images/kx404_.png)

不好的数据的边界

![不好的数据的边界](/images/kx405_.png)

统计流程

![统计流程](/images/kx406_.png)


## 下一课

[Training versus Testing](/note/MLF/mlf05.md)