# Feasibility of Learning 机器学习可行性
## 上节回顾

重点关注：具有`具体特征`的`填鸭式`(batch)的`监督式`数据的`二元分类或回归`

## Learning is Impossible? 学习似乎不可能 - 样本外绝对符合 NFL

- 第一种情景描述，根据不同特征进行分类，可以得到不同的分类结果

![分不准](/images/kx101_.png)

- 第二种情景描述，在已知数据集 D 上，g≈f 成立；在 D 以外的未知数据集上，g≈f 不一定成立

![局限](/images/kx102_.png)

![局限解答](/images/kx103_.png)

解释：

在 D 以外的数据中更接近目标函数似乎是做不到的，只能保证对 D 有很好的分类结果 - No Free Lunch 定理

> 通常一个学习算法比另一个更“优越”，只是针对特定的问题，先验信息，数据的分布，训练样本的数目，代价或奖励函数

结论：**在训练集 D 以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的，除非加上一些假设条件**

## Probability to the Rescue 可能性推断 - 样本外很可能逼近正确

既然学习对样本以外未知的目标函数 f 很难推断，那么是否在其他场景可以推断一些未知呢？

![bin题设](/images/kx201_.png)

如何推断橙色珠子的概率？

![问题描述](/images/kx202_.png)

那么样本内的概率`ν`与样本外的概率`μ`有什么关系呢？
- 主观上，可能没关系(即使罐子橙珠非常多，拿出的样本中橙珠极其少)
- 概率上，很可能有关系(两者很可能相近)

![可能性与概率](/images/kx203_.png)

那么用公式([霍夫丁不等式](/note/SC/霍夫丁不等式.md))如何描述呢？

![霍夫丁](/images/kx204_.png)

显然，`ν ≈ μ`极有可能是正确的
- N 足够大，ϵ 足够小，从而上述概率(表示两者相近的可能性)足够大
- 不依赖未知的`μ`

![推断](/images/kx205_.png)

那么 N 足够大时，就极有可能根据已知的`ν`推断未知的`μ`

计算一下，假设 `μ=0.4`，那么从罐子中取一个大小为 10 的样本，其橙珠的概率`ν≤0.1`的概率最大是多少？

**0.33** ， 也就是说两者相差很大的可能性的上界也很小，反过来说，两者相差很小的可能性非常大

## Connection to Learning 转到机器学习 - 对特定 h，样本中误差小的可能性验证 
如果样本 N 够大，且是独立同分布的，就可以根据**样本内 h(x)≠y_n 的概率**(`E_in`)来推断**样本外 h(x)≠f(x) 的概率**(`E_out`)

![错误率](/images/kx300_.png)

那么怎么根据 E_in≈E_out 推断 g≈f 呢？

![推断](/images/kx301_.png)

用公式表示就是

![公式](/images/kx302_.png)

也就是说 E_in≈E_out 并且 E_in 很小时，E_out 也会很小

从而可以根据**样本中 h(x)=y_n 的概率**来推断**样本外 h(x)=f(x) 的概率**

**因此，当 E_in 很小时，E_out 也会很小，从而很可能 h≈f**

那么我们来验证一个 h
- 如果对特定的 h，样本中分类出错率很小时，那么选择这个 h 作为 g 时，g=f 很可能
- 如果算法 A 被限制选择这个 h，不能保证样本中分类出错率足够小，那么 g≠f 很可能

![验证h](/images/kx303_.png)

然而现实情况是，A 有很多选择，也就是说可以选择出最好的 h，使得 E_in 很小，这样的话 g=f 就很可能

验证一个 h 的流程 - 固定 h，使用新数据进行预测，看看验证出错率是多少

![验证流程](/images/kx304_.png)

## Connection to Real Learning 转到实际学习 - M 有限且样本中误差很小，学习很可能

假设有多个 h，其中某个罐子的样本全是绿的，那是不是应该选择这个罐子呢？ 

![多个h](/images/kx401_.png)

先来看一个抛硬币游戏

![抛硬币游戏](/images/kx402_.png)

也就是说当罐子数目很多或者抛硬币的人数很多的时候，可能带来 Bad Sample

Bad Sample 与 Bad Data 的概念

![不好的数据和标签](/images/kx403_.png)

根据许多次抽样的到的不同的数据集D，Hoeffding's inequality 保证了大多数的 D 都是比较好的情形，但是也有可能出现 Bad Data，这是小概率事件

![对多个h来说不好的数据](/images/kx404_.png)

那么这种小概率事件的边界是什么呢？

![不好的数据的边界](/images/kx405_.png)

其中，M 是 hypothesis 的个数，N 是样本 D 的数量， ϵ 是参数

该边界表明，当 M 有限，且 N 足够大的时候，Bad Data 出现的概率就更低了

所以，如果 hypothesis 的个数 M 是有限的，N 足够大，那么通过演算法 A 任意选择一个 g，都有 样本中分类出错率 ≈ 样本外分类出错率 成立

同时，如果找到一个 g，使样本中出错率≈0 ，就很可能保证样本外出错率≈0。至此，就证明了机器学习是可行的

验证流程

![流程](/images/kx406_.png)

但是，M 无限大时怎么办？

## 下一课

[Training versus Testing](mlf05.md)