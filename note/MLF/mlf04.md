# Feasibility of Learning 学习的可行性
## 上节回顾

重点关注：具有`具体特征`的`填鸭式`(batch)的`监督式`数据的`二元分类或回归`

## Learning is Impossible? 学习似乎不可能 - 样本外绝对符合 NFL

- 第一种情景描述，根据不同特征进行分类，可以得到不同的分类结果

![分不准](/images/kx101_.png)

- 第二种情景描述，在已知数据集 D 上，g≈f 成立；在 D 以外的未知数据集上，g≈f 不一定成立

![局限](/images/kx102_.png)

![局限解答](/images/kx103_.png)

解释：

在 D 以外的数据中更接近目标函数似乎是做不到的，只能保证对 D 有很好的分类结果 - No Free Lunch 定理

> 通常一个学习算法比另一个更“优越”，只是针对特定的问题，先验信息，数据的分布，训练样本的数目，代价或奖励函数

结论：**在训练集 D 以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的，除非加上一些假设条件**

## Probability to the Rescue 可能性推断 - 样本外很可能逼近正确

既然学习对样本以外未知的目标函数 f 很难推断，那么是否在其他场景可以推断一些未知呢？

![bin题设](/images/kx201_.png)

如何推断橙色珠子的概率？

![问题描述](/images/kx202_.png)

那么样本内的概率`ν`与样本外的概率`μ`有什么关系呢？
- 主观上，可能没关系(即使罐子橙珠非常多，拿出的样本中橙珠极其少)
- 概率上，很可能有关系(两者很可能相近)

![可能性与概率](/images/kx203_.png)

那么用公式([霍夫丁不等式](/note/SC/霍夫丁不等式.md))如何描述呢？ - 相差很大的概率很小很小

![霍夫丁](/images/kx204_.png)

显然，`ν ≈ μ`概率很大(N 足够大)

![推断](/images/kx205_.png)

那么 N 足够大时，就极有可能根据已知的`ν`推断未知的`μ`

## Connection to Learning 转到学习 - 对固定 h，霍夫丁 E_in≈E_out <-> g≈f

如果样本 N 够大，且 X 是独立同分布的，就可以根据**样本内 h(X)≠y_n 的概率**(`E_in`)来推断**样本外 h(X)≠f(X) 的概率**(`E_out`)

![错误率](/images/kx300_.png)

也就是说 E_in≈E_out? <-> g≈f？

![推断](/images/kx301_.png)

用公式表示就是

![公式](/images/kx302_.png)

也就是说 E_in≈E_out 并且 E_in≈0 时，E_out≈0

从而可以根据**样本中 h(x)=y_n 的概率**来推断**样本外 h(x)=f(x) 的概率**

**因此，E_in≈E_out <-> g≈f**

那么我们来验证，对于固定的那个 h
- 如果对特定的 h，E_in≈0 时，那么选择这个 h 作为 g 时，g=f 很可能
- 如果算法 A 被限制只能选择这个 h，E_in≈0 的概率很小，那么 g≠f 很可能

![验证h](/images/kx303_.png)

然而真正的学习是 A 有很多选择，也就是说可以选择出最好的 h，使得 E_in≈0，这样的话 g=f 就很可能

一个 h 的验证(还不是学习)流程 - 固定 h，使用新数据(验证数据集)进行预测，看看验证出错率是多少

![验证流程](/images/kx304_.png)

## Connection to Real Learning 转到真正的学习 - M 有限且 E_in≈0，学习很可能

假设有多个 h，其中某个罐子的样本全是绿的，那是不是应该选择这个罐子呢？ 

![多个h](/images/kx401_.png)

先来看一个抛硬币游戏

![抛硬币游戏](/images/kx402_.png)

也就是说当罐子数目很多或者抛硬币的人数很多的时候，可能带来 Bad Sample

Bad Sample 意味着 E_in 与 E_out 相差太大

Bad Sample 与 Bad Data 的存在

![不好的数据和标签](/images/kx403_.png)

根据许多次抽样得到的不同的样本 D ，`霍夫丁不等式`保证了大多数的 D 都是比较好的情形，但是也有可能出现 Bad Data，这是小概率事件

![对多个h来说不好的数据](/images/kx404_.png)

那么这种小概率事件的联合边界是什么呢？

![不好的数据的边界](/images/kx405_.png)

其中，M 是 h 的个数，N 是样本 D 的数量， ϵ 是容忍度

该联合边界表明
- 当 M 有限，且 N 足够大的时候，Bad Data 出现的概率就更低了
- 不依赖于 E_out
- E_in=E_out 很可能，且于算法 A 无关

也就是说，如果M 是有限的，N 足够大，那么通过演算法 A 任意选择一个 g，都有 E_in ≈ E_out

同时，如果找到一个 g，使得E_in≈0，那么很可能 E_out≈0。至此，就证明了机器学习是可行的

下面是基于统计学的学习流程

![流程](/images/kx406_.png)

但是，M 无限大时怎么办？

## 下一课

[Training versus Testing](mlf05.md)