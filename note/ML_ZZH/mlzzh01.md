# 绪论
## 什么是机器学习

我们根据过去的经验来判断明天的天气，根据购买经验来挑选一个好瓜，`通过对以往经验的利用，就能对新的情况做出有效的决策`。机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。

## 基本术语

假设我们收集了一批西瓜的数据，例如:

    (色泽=青绿;根蒂=蜷缩;敲声=浊响)，
    (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)，
    (色泽=浅自;根蒂=硬挺;敲声=清脆)
    ……

每对括号内是一个西瓜的记录，则有如下定义:

- 这组记录的集合:**数据集(data set)**
- 其中每一条记录是关于一个事件或对象的描述:**一个示例(instance)或样本(sample)**
- 反映事物或对象在某方面的表现或性质的事项(如色泽或敲声):**特征(feature)或属性(attribute)**
- 属性上的取值:**属性值(attribute value)**
- 属性张成的空间:**属性空间 (attribute space)、 样本空间 (samp1e space)或输入空间(input space)**

`如我们把"色泽" "根蒂" "敲声"作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置`

- 由于空间中的每个点对应一个坐标向量，因此一个示例也称为**一个特征向量(feature vector)**
- 一个样本的特征数:**维数(dimensionality)**
- 从数据中学得模型的过程:**学习(learning)或训练(training)**
- 训练过程中使用的数据:**训练数据(training data)**
- 其中每一个样本:**训练样本(training sample)**
- 训练样本组成的集合:**训练集(training set)**
- 学得模型对应了关于数据的某种潜在的规律:**假设(hypothesis)**
- 这种潜在的规律自身:**真相(ground-truth)**

`学习的过程就是为了找出或逼近真相`

- 关于示例结果的信息:**标签(label)**
- 拥有了标签信息的示例:**样例(example)**
- 所有标签的集合:**标签空间(label space)或(output space)**
- 若欲预测的是离散值，此类学习任务:**分类(classification)**
- 欲预测的是连续值，此类学习任务:**回归(regression)**
- 只涉及两个类别的分类任务:**二分类(binary classification)**
- 通常称其中一个类为**正类 (positive class)**，另一个类为**反类 (negative class)**
- 涉及多个类别的分类任务:**多分类(multi-class classification)**

一般地，预测任务是希望通过对训练集进行学习，建立一个从输入空间到输出空间的映射

- 学得模型后，使用其进行预测的过程:**测试(testing)**
- 被预测的样本:**测试样本(testing sample)**
- 学得模型适用于新样本的能力:**泛化(generalization)能力**
- 通常假设样本空间中全体样本服从一个未知分布，我们获取的样本都是独立地从这个分布上采样获得的:**独立同分布(independent and identically distributed)**

## 假设空间

`归纳(induction)与演绎(deduction)`是科学推理的两大基本手段。前者是从特殊到一般的"泛化"(generalization)过程，即从具体的事实归结出一般性规律;后者则是从一般到特殊的"特化"(specialization)过程，即从基础原理推演出具体状况。

归纳学习有狭义与广义之分。广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念(concept)，因此亦称为"概念学习"或"概念形成"。

概念学习中最基本的是布尔概念学习。即对"是"、"不是"这样的可表示为 0/1 布尔值的目标概念的学习。

![概念学习](/images/zzh01.png)

这里要学习的目标是"好瓜"，暂且假设"好瓜"可由"色泽" "根蒂" "敲声"这三个因素完全确定，换言之，只要某个瓜的这三个属性取值明确了，我们就能判断出它是不是好瓜。于是，我们学得的将是"好瓜是某种色泽、某 种根蒂、某种敲声的瓜"这样的概念，用布尔表达式写出来则是

    好瓜 <-> (色泽=?) ^ (根蒂=?) ^ (敲声=?)

我们学习的目的是"泛化"，即通过对训练集中瓜的学习以获得对没见过的瓜进行判断的能力。

我们可以把学习过程看作一个在所有假设 (hypothesis)组成的空间中进行搜索的过程，搜索目标是找到与训练集"匹配"(fit)的假设，即能够将训练集中的瓜判断正确的假设。假设的表示一旦确定，假设空间及其规模大小就确定了。

现实问题中我们常面临很大的假设空间?但学习过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的"假设集合"，我们称之为"版本空间"(version space)。

## 归纳偏好(权重)

通过学习得到的模型对应了假设空间中的一个假设。而存在多个假设与训练集一致时，我们需要考虑偏好。

![多个假设](/images/zzh02.png)

机器学习算法在学习过程中对某种类型假设的偏好，称为"归纳偏好" (inductive bias)，或简称为"偏好"。

任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上"等效"的假设所迷惑，而无法产生确定的学习结果。

归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或"价值观"。

"奥卡姆剃刀" (Occam's razor)是一种常用的、自然科学研究中最基本的原则，即"若有多个假设与观察一致，则选最简单的那个"。

事实上，归纳偏好对应了学习算法本身所做出的关于"什么样的模型更好"的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法能否取得好的性能。

然而，对于一个学习算法 A，若它在某些问题上比学习算法 B 好，则必然存在另一些问题，在那里 B 比 A 好。有趣的是，这个结论对任何算法均成立，哪怕是把本书后面将要介绍的一些聪明算法作为 A，而将"随机胡猜"这样的笨拙算法作为 B。

![NFL](/images/zzh03.png)

我们需注意到， NFL 定理有一个重要前提：所有"问题"出现的机会相同、或所有问题同等重要。但实际情形并不是这样。很多时候，我们只关注自己正在试图解决的问题(例如某个具体应用任务)，希望为它找到一个解决方案， 至于这个解决方案在别的问题、甚至在相似的问题上是否为好方案，我们并不关心。

所以， NFL 定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论"什么学习算法更好"毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的相对优劣，必须要针对具体的学习问题；在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用。

## 下一章

[模型评估与选择](mlzzh02.md)