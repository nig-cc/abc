# 神经网络基础

## 二元分类

二分类就是输出 y 只有两个离散值

## 逻辑回归
Logistic 回归是一个用于二分分类的算法

Logistic 回归中使用的参数如下：

![参数](/images/dl0110.png)

Logistic 回归可以看作是一个非常小的神经网络。下图是一个典型例子：

![典型例子](/images/dl0111.png)

## 逻辑回归损失函数

损失函数（loss function）用于衡量预测结果与真实值之间的误差。

最简单的损失函数定义方式为平方差损失：

![平方差损失](/images/dl0112.png)

但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论的优化问题会变成非凸的(多个局部最优解)

一般使用

![一般使用](/images/dl0113.png)

损失函数是在单个训练样本中定义的，它衡量了在单个训练样本上的表现

而代价函数（cost function，或者称作成本函数）衡量的是在全体训练样本上的表现，即衡量参数 w 和 b 的效果

![代价函数](/images/dl0114.png)

## 梯度下降法

函数的梯度（gradient）指出了函数的最陡增长方向

按梯度的方向走，函数增长得就越快

模型的训练目标即是寻找合适的 w 与 b 以最小化代价函数值

成本函数可视化

![成本函数](/images/dl0115.png)

可以看到，成本函数 J 是一个凸函数，这样的代价函数就保证无论我们初始化模型参数如何，都能够寻找到合适的最优解

参数 w 的更新公式为

![w更新公式](/images/dl0116.png)

参数 b 的更新公式为

![b更新公式](/images/dl0117.png)

其中 α 表示学习速率，即每次更新的 w 的步伐长度

## 计算图

神经网络中的计算即是由多个计算网络输出的前向传播与计算梯度的后向传播构成

反向传播（Back Propagation）是计算最终值相对于某个特征变量的导数时，利用计算图中上一步的结点定义

![计算图正向传播](/images/dl0118.png)

## 计算图的导数计算

反向传播（Back Propagation），即计算输出对输入的偏导数

![计算图正向传播](/images/dl0119.png)

## 逻辑回归中的梯度下降计算

假设输入的特征向量维度为 2，即输入参数共有 x1, w1, x2, w2, b 这五个。可以推导出如下的计算图：

![计算图1](/images/dl0120.png)

首先反向求出 L 对于 a 的导数：

![导数a](/images/dl0121.png)

然后继续反向求出 L 对于 z 的导数：

![导数z](/images/dl0122.png)

依此类推求出最终的损失函数相较于原始参数的导数之后，根据如下公式进行参数更新：

![参数更新](/images/dl0123.png)

接下来我们需要将对于单个用例的损失函数扩展到整个训练集的代价函数：

![代价函数](/images/dl0124.png)

我们可以对于某个权重参数 w1，其导数计算为：

![其导数计算](/images/dl0125.png)

完整的 Logistic 回归中某次训练的流程如下，这里仅假设特征向量的维度为 2：

![流程](/images/dl0126.png)

然后对 w1、w2、b 进行迭代

上述过程在计算时有一个缺点：你需要编写两个 for 循环

如果有大量特征，在代码中显式使用 for 循环会使算法很低效

向量化可以用于解决显式使用 for 循环的问题

## 向量化

不用显式 for 循环，实现 Logistic 回归的梯度下降一次迭代

![不用显式循环](/images/dl0127.png)

正向和反向传播尽管如此，多次迭代的梯度下降依然需要 for 循环

