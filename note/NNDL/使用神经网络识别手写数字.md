# 第1章 使用神经网络识别手写数字
The human visual system is one of the wonders of the world. Consider the following sequence of handwritten digits:
人类视觉系统是世界奇迹之一。考虑以下手写数字序列：

![](images/digits.png)

Most people effortlessly recognize those digits as 504192. That ease is deceptive. In each hemisphere of our brain, humans have a primary visual cortex, also known as V1, containing 140 million neurons, with tens of billions of connections between them. And yet human vision involves not just V1, but an entire series of visual cortices - V2, V3, V4, and V5 - doing progressively more complex image processing. We carry in our heads a supercomputer, tuned by evolution over hundreds of millions of years, and superbly adapted to understand the visual world. Recognizing handwritten digits isn't easy. Rather, we humans are stupendously, astoundingly good at making sense of what our eyes show us. But nearly all that work is done unconsciously. And so we don't usually appreciate how tough a problem our visual systems solve.
大多数人毫不费力地将这些数字识别为504192.这种容易具有欺骗性。在我们大脑的每个半球，人类都有一个初级视觉皮层，也称为V1，含有1.4亿个神经元，它们之间有数百亿个连接。然而，人类视觉不仅涉及V1，还涉及整个系列的视觉皮层--V2，V3，V4和V5 - 逐步进行更复杂的图像处理。我们的头脑是一台超级计算机，经过数亿年的演变调整，并且非常适应了解视觉世界。识别手写数字并不容易。相反，我们人类是惊人的，非常善于理解我们的眼睛向我们展示的东西。但几乎所有这些工作都是在无意识中完成的。因此，我们通常不会理解视觉系统解决问题的难度。

The difficulty of visual pattern recognition becomes apparent if you attempt to write a computer program to recognize digits like those above. What seems easy when we do it ourselves suddenly becomes extremely difficult. Simple intuitions about how we recognize shapes - "a 9 has a loop at the top, and a vertical stroke in the bottom right" - turn out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions and caveats and special cases. It seems hopeless.
如果您尝试编写计算机程序来识别上述数字，那么视觉模式识别的难度就会变得很明显。当我们自己做的时候看起来很容易变得非常困难。关于我们如何识别形状的简单直觉 - “一个9在顶部有一个环，一个在右下方有一个垂直笔划” - 结果并不是那么简单的算法表达。当你试图使这些规则变得精确时，你很快就会陷入异常和警告以及特殊情况。这似乎毫无希望。

Neural networks approach the problem in a different way. The idea is to take a large number of handwritten digits, known as training examples,
神经网络以不同的方式解决问题。我们的想法是采用大量手写数字，称为训练样例，

![](images/mnist_100_digits.png)

and then develop a system which can learn from those training examples. In other words, the neural network uses the examples to automatically infer rules for recognizing handwritten digits. Furthermore, by increasing the number of training examples, the network can learn more about handwriting, and so improve its accuracy. So while I've shown just 100 training digits above, perhaps we could build a better handwriting recognizer by using thousands or even millions or billions of training examples.
然后开发一个可以从这些训练样例中学习的系统。换句话说，神经网络使用示例来自动推断用于识别手写数字的规则。此外，通过增加训练样本的数量，网络可以更多地了解手写，从而提高其准确性。因此，虽然我上面只显示了100个训练数字，但也许我们可以通过使用数千甚至数百万或数十亿的训练样例来构建更好的手写识别器。

In this chapter we'll write a computer program implementing a neural network that learns to recognize handwritten digits. The program is just 74 lines long, and uses no special neural network libraries. But this short program can recognize digits with an accuracy over 96 percent, without human intervention. Furthermore, in later chapters we'll develop ideas which can improve accuracy to over 99 percent. In fact, the best commercial neural networks are now so good that they are used by banks to process cheques, and by post offices to recognize addresses.
在本章中，我们将编写一个计算机程序，实现一个学习识别手写数字的神经网络。该程序只有74行，并没有使用特殊的神经网络库。但是这个简短的程序可以识别精度超过96％的数字，无需人工干预。此外，在后面的章节中，我们将开发出可以将准确度提高到99％以上的想法。实际上，最好的商业神经网络现在非常好，银行可以使用它们来处理支票，并通过邮局来识别地址。

We're focusing on handwriting recognition because it's an excellent prototype problem for learning about neural networks in general. As a prototype it hits a sweet spot: it's challenging - it's no small feat to recognize handwritten digits - but it's not so difficult as to require an extremely complicated solution, or tremendous computational power. Furthermore, it's a great way to develop more advanced techniques, such as deep learning. And so throughout the book we'll return repeatedly to the problem of handwriting recognition. Later in the book, we'll discuss how these ideas may be applied to other problems in computer vision, and also in speech, natural language processing, and other domains.
我们专注于手写识别，因为它是学习神经网络的一个很好的原型问题。作为原型，它有一个最佳点：它具有挑战性 - 识别手写数字并不是一件小事 - 但要求极其复杂的解决方案或巨大的计算能力并不困难。此外，它是开发更高级技术的好方法，例如深度学习。因此，在整本书中，我们将反复回到手写识别问题。在本书的后面，我们将讨论这些想法如何应用于计算机视觉中的其他问题，以及语音，自然语言处理和其他领域。

Of course, if the point of the chapter was only to write a computer program to recognize handwritten digits, then the chapter would be much shorter! But along the way we'll develop many key ideas about neural networks, including two important types of artificial neuron (the perceptron and the sigmoid neuron), and the standard learning algorithm for neural networks, known as stochastic gradient descent. Throughout, I focus on explaining why things are done the way they are, and on building your neural networks intuition. That requires a lengthier discussion than if I just presented the basic mechanics of what's going on, but it's worth it for the deeper understanding you'll attain. Amongst the payoffs, by the end of the chapter we'll be in position to understand what deep learning is, and why it matters.
当然，如果本章的目的只是编写一个识别手写数字的计算机程序，那么这一章就会短得多！但在此过程中，我们将开发许多关于神经网络的关键思想，包括两种重要类型的人工神经元（感知器和乙状结肠神经元），以及神经网络的标准学习算法，称为随机梯度下降。在整个过程中，我专注于解释为什么事情以他们的方式完成，以及建立你的神经网络直觉。这需要更长时间的讨论，而不是我刚刚介绍了正在发生的事情的基本机制，但是对于你将获得更深刻的理解是值得的。在收益中，到本章结尾，我们将能够理解深度学习是什么。

## Perceptrons 感知器
What is a neural network? To get started, I'll explain a type of artificial neuron called a perceptron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. Today, it's more common to use other models of artificial neurons - in this book, and in much modern work on neural networks, the main neuron model used is one called the sigmoid neuron. We'll get to sigmoid neurons shortly. But to understand why sigmoid neurons are defined the way they are, it's worth taking the time to first understand perceptrons.
什么是神经网络？首先，我将解释一种称为感知器的人工神经元。感知器是 由科学家弗兰克罗森布拉特在20世纪50年代和60年代开发的 ，受到沃伦麦卡洛克和 沃尔特皮茨的早期工作 的启发 。今天，使用其他模型的人工神经元更为常见 - 在本书中，在神经网络的许多现代工作中，使用的主要神经元模型是一个称为S形神经元的模型 。我们很快就会得到sigmoid神经​​元。但要理解为什么乙状结肠神经元的定义方式如此，值得花时间先了解感知器。

So how do perceptrons work? A perceptron takes several binary inputs, x1,x2,…
那么感知器如何工作呢？感知器需要几个二进制输入 x1，x2，...，并生成一个二进制输出：

![](images/tikz0.png)

In the example shown the perceptron has three inputs, x1,x2,x3. In general it could have more or fewer inputs. Rosenblatt proposed a simple rule to compute the output. He introduced weights, w1,w2,…, real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined by whether the weighted sum ∑w_jx_j is less than or greater than some threshold value. Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms:
在所示的示例中，感知器有三个输入x1，x2，x3。一般来说，它可能有更多或更少的输入。罗森布拉特提出了一个计算输出的简单规则。他介绍了*权重*，w1，w2，...，实数表示各输入对输出的重要性。神经元的输出，0或1，是由加权和∑w_jx_j小于或大于某个阈值决定的。就像权重一样，阈值是一个实数，它是神经元的参数。用更精确的代数术语来表达它：

![](images/gzq01.png)

That's all there is to how a perceptron works!
这就是感知器的工作方式！

That's the basic mathematical model. A way you can think about the perceptron is that it's a device that makes decisions by weighing up evidence. Let me give an example. It's not a very realistic example, but it's easy to understand, and we'll soon get to more realistic examples. Suppose the weekend is coming up, and you've heard that there's going to be a cheese festival in your city. You like cheese, and are trying to decide whether or not to go to the festival. You might make your decision by weighing up three factors:
这是基本的数学模型。您可以考虑感知器的一种方式是，它是一种通过权衡证据来做出决策的设备。让我举个例子。这不是一个非常现实的例子，但它很容易理解，我们很快就会得到更实际的例子。假设周末即将来临，你已经听说你所在城市将举办奶酪节。你喜欢奶酪，并试图决定是否去参加电影节。您可以通过权衡三个因素来做出决定：

- Is the weather good?
- Does your boyfriend or girlfriend want to accompany you?
- Is the festival near public transit? (You don't own a car).

- 天气好吗？
- 你的男朋友或女朋友想陪你吗？
- 公共交通附近有节日吗？（你没有车）。

We can represent these three factors by corresponding binary variables x1,x2, and x3. For instance, we'd have x1=1 if the weather is good, and x1=0 if the weather is bad. Similarly, x2=1 if your boyfriend or girlfriend wants to go, and x2=0 if not. And similarly again for x3 and public transit.
我们可以通过相应的二元变量x来表示这三个因子 1，x2和x3。例如，我们有x1= 1如果天气好的话，和x1= 0如果天气不好 同样，x2= 1如果你的男朋友或女朋友想去，和x2= 0如果不。并且类似地再次为x3 和公共交通。

Now, suppose you absolutely adore cheese, so much so that you're happy to go to the festival even if your boyfriend or girlfriend is uninterested and the festival is hard to get to. But perhaps you really loathe bad weather, and there's no way you'd go to the festival if the weather is bad. You can use perceptrons to model this kind of decision-making. One way to do this is to choose a weight w1=6 for the weather, and w2=2 and w3=2 for the other conditions. The larger value of w1 indicates that the weather matters a lot to you, much more than whether your boyfriend or girlfriend joins you, or the nearness of public transit. Finally, suppose you choose a threshold of 5 for the perceptron. With these choices, the perceptron implements the desired decision-making model, outputting 1 whenever the weather is good, and 0 whenever the weather is bad. It makes no difference to the output whether your boyfriend or girlfriend wants to go, or whether public transit is nearby.

By varying the weights and the threshold, we can get different models of decision-making. For example, suppose we instead chose a threshold of 3. Then the perceptron would decide that you should go to the festival whenever the weather was good or when both the festival was near public transit and your boyfriend or girlfriend was willing to join you. In other words, it'd be a different model of decision-making. Dropping the threshold means you're more willing to go to the festival.

Obviously, the perceptron isn't a complete model of human decision-making! But what the example illustrates is how a perceptron can weigh up different kinds of evidence in order to make decisions. And it should seem plausible that a complex network of perceptrons could make quite subtle decisions:

![](images/tikz1.png)

In this network, the first column of perceptrons - what we'll call the first layer of perceptrons - is making three very simple decisions, by weighing the input evidence. What about the perceptrons in the second layer? Each of those perceptrons is making a decision by weighing up the results from the first layer of decision-making. In this way a perceptron in the second layer can make a decision at a more complex and more abstract level than perceptrons in the first layer. And even more complex decisions can be made by the perceptron in the third layer. In this way, a many-layer network of perceptrons can engage in sophisticated decision making.
Incidentally, when I defined perceptrons I said that a perceptron has just a single output. In the network above the perceptrons look like they have multiple outputs. In fact, they're still single output. The multiple output arrows are merely a useful way of indicating that the output from a perceptron is being used as the input to several other perceptrons. It's less unwieldy than drawing a single output line which then splits.

Let's simplify the way we describe perceptrons. The condition ∑jwjxj>threshold is cumbersome, and we can make two notational changes to simplify it. The first change is to write ∑jwjxj as a dot product, w⋅x≡∑jwjxj, where w and x are vectors whose components are the weights and inputs, respectively. The second change is to move the threshold to the other side of the inequality, and to replace it by what's known as the perceptron's bias, b≡−threshold. Using the bias instead of the threshold, the perceptron rule can be rewritten:

You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get the perceptron to fire. For a perceptron with a really big bias, it's extremely easy for the perceptron to output a 1. But if the bias is very negative, then it's difficult for the perceptron to output a 1. Obviously, introducing the bias is only a small change in how we describe perceptrons, but we'll see later that it leads to further notational simplifications. Because of this, in the remainder of the book we won't use the threshold, we'll always use the bias.

I've described perceptrons as a method for weighing evidence to make decisions. Another way perceptrons can be used is to compute the elementary logical functions we usually think of as underlying computation, functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight −2, and an overall bias of 3. Here's our perceptron:

![](images/tikz2.png)